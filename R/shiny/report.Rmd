---
title: "Assignment: Final Project"
author: "Tushar Kapoor"
date: "June 17, 2017"
output: html_document
---

### Summary

This report, submitted in partial fulfillment of the requirements of the Coursera course, "Data Science Capstone", concerns the R package "wordprediction", which contains functions for an auto-completion model that predicts the next word to be typed based on a word or phrase previously typed.  

Auto-completion is a common function on mobile devices. As a user types, an auto-completion function presents that user with possible completions to the current word being typed or probable words that could follow the current word or phrase after it is typed. The package "wordprediction" provides the latter function.

### Exploratory Analysis

```{r libraries, cache = TRUE, echo = FALSE, include = FALSE}

# Load the necessary libraries.

library("tm")
library("ngramrr")

```



#### Data File

```{r file, cache = TRUE, echo = FALSE, include = FALSE}

#' Download Data File
#'
#' Downloads a data file.
#'
#' @param location URL of the data file to be used formatted as a character.
#' @param data_file name of the data file to be used formatted as a character.
#' @author Tushar Kapoor
#' @details
#' This function downloads a data file from a specified location URL for use in
#' the package "wordprediction".
#' @export
#' @importFrom utils download.file

download_data_file <- function(location, data_file) {

    if (!file.exists(data_file)) {

        download.file(
            url = location,
            destfile = data_file,
            method = "curl"
        )

    }

}


#' Uncompress Data File
#'
#' Uncompresses a data file
#'
#' @param location local directory created by unpacking the data file.
#' @param data_file name of the data file to be used formatted as a character.
#' @author Tushar Kapoor
#' @details
#' This function uncompresses a data file for use in the package
#' "wordprediction".
#' @export
#' @importFrom utils unzip

uncompress_data_file <- function(location, data_file) {

    if (!dir.exists(location)) {

        unzip(data_file)

    }

}


#' Sample Data File
#'
#' Samples a data file.
#'
#' @author Tushar Kapoor
#' @details
#' This function samples 1000 lines each from the data files via Mac OS X shell.
#' @export

sample_data_file <- function() {

    if (!file.exists("./final/de_DE/de_DE.blogs.txt")) {

        system(
            "
            cd ./final/de_DE;
            gshuf -n 1000 de_DE.blogs.txt > de_DE.blogs.sample.txt;
            gshuf -n 1000 de_DE.news.txt > de_DE.news.sample.txt;
            gshuf -n 1000 de_DE.twitter.txt > de_DE.twitter.sample.txt;
            rm de_DE.blogs.txt;
            rm de_DE.news.txt;
            rm de_DE.twitter.txt;
            cd ..;
            cd ./final/en_US;
            gshuf -n 1000 en_US.blogs.txt > en_US.blogs.sample.txt;
            gshuf -n 1000 en_US.news.txt > en_US.news.sample.txt;
            gshuf -n 1000 en_US.twitter.txt > en_US.twitter.sample.txt;
            rm en_US.blogs.txt;
            rm en_US.news.txt;
            rm en_US.twitter.txt;
            cd ..;
            cd ./final/fi_FI;
            gshuf -n 1000 fi_FI.blogs.txt > fi_FI.blogs.sample.txt;
            gshuf -n 1000 fi_FI.news.txt > fi_FI.news.sample.txt;
            gshuf -n 1000 fi_FI.twitter.txt > fi_FI.twitter.sample.txt;
            rm fi_FI.blogs.txt;
            rm fi_FI.news.txt;
            rm fi_FI.twitter.txt;
            cd ..;
            cd ./final/ru_RU;
            gshuf -n 1000 ru_RU.blogs.txt > ru_RU.blogs.sample.txt;
            gshuf -n 1000 ru_RU.news.txt > ru_RU.news.sample.txt;
            gshuf -n 1000 ru_RU.twitter.txt > ru_RU.twitter.sample.txt;
            rm ru_RU.blogs.txt;
            rm ru_RU.news.txt;
            rm ru_RU.twitter.txt;
            cd ..; cd ..;
            "
        )

    }

}

```

In order to build a function that can provide word-prediction, a predictive model is needed.  Such models use known content to predict unknown content.  For this package, that content comes from the HC Corpora collection, which is "a collection of corpora for various languages freely available to download" (Christensen, n.d.). The version used was obtained from an archive maintained at Coursera (Leek, Peng, Caffo, & Johns Hopkins University, n.d.).  The file included three text document collections, blogs, news feeds, and tweets, in four languages, German, English, Finnish, and Russian, of which only the English collections were used.  The files were too large to be manipulated using a home computer (e.g., the downloaded ZIP file was 575 MB).  Therefore, 1000 lines were randomly sampled from each collection using a Mac OS X (Version x86_64-apple-darwin13.4.0) terminal application before loading for analysis in RStudio (Version 0.99.892) running the R statistical programming language (Version 3.2.3).

#### Data Structure

```{r structure, cache = TRUE, echo = FALSE, include = FALSE}

#' Read Data
#'
#' Reads data into a corpus.
#'
#' @param language
#' a character giving the language as IETF language tags "de" for German, "en"
#' for English, "fi" for Finnish, or "ru" for Russian.
#' @return corpus a volatile text corpus as a "tm" package object
#' @author Michael David Gill
#' @details
#' This function reads from a data file as an object of type "corpus" from the
#' "tm" ppackage. It chooses the files corresponding to the language indicated
#' by the language parameter.
#' @export
#' @importFrom tm VCorpus
#' @importFrom tm DirSource
#' @importFrom tm readPlain

read_data <- function(language) {

    if (language == "de") {

        if (!exists("corpus")) {

            VCorpus(
                DirSource("C:/Capstone/coursera-final-project-submission/R/final/de_DE"),
                readerControl = list(reader = readPlain, language  = "de")
            )

        }

    } else if (language == "en") {

        if (!exists("corpus")) {

            VCorpus(
                DirSource("C:/Capstone/coursera-final-project-submission/R/final/en_US"),
                readerControl = list(reader = readPlain)
            )

        }

    } else if (language == "fi") {

        if (!exists("corpus")) {

            VCorpus(
                DirSource("C:/Capstone/coursera-final-project-submission/R/final/fi_FI"),
                readerControl = list(reader = readPlain, language  = "fi")
            )

        }

    } else if (language == "ru") {

        if (!exists("corpus")) {

            VCorpus(
                DirSource("C:/Capstone/coursera-final-project-submission/R/final/ru_RU"),
                readerControl = list(reader = readPlain, language  = "ru")
            )

        }

    }

}

```

This package uses the data stuctures described in Feinerer, Hornik, and Meyer (2008) from the Text Mining Package (Version 0.6-2; Feinerer, Hornik, & Artifex Software, Inc., 2015) "tm".  In these structures, text document collections are organized into corpora, the basic objects to be manipulated by the word-prediction function.  Accordingly, the HC Corpora data file were loaded as tm corpora.

#### Data Cleaning

```{r cleaning, cache = TRUE, echo = FALSE, include = FALSE}

#' Preprocess Corpus
#'
#' Preprocesses the corpus.
#'
#' @param corpus a volatile text corpus as a "tm" package object
#' @return a preprocessed volatile text corpus as a "tm" package object
#' @author Michael David Gill
#' @details
#' This function preprocesses a corpus by removing punctuation, numbers, and
#' English stopwords, and stripping whitespace.
#' @export
#' @importFrom tm tm_map
#' @importFrom tm removePunctuation
#' @importFrom tm removeNumbers
#' @importFrom tm content_transformer
#' @importFrom tm stripWhitespace
#' @importFrom tm removeWords
#' @importFrom tm stopwords

preprocess_corpus <- function(corpus) {

    # Remove punctuation from text.
    corpus_preprocessed <- tm_map(corpus, removePunctuation)

    # Remove numbers from text.
    corpus_preprocessed <- tm_map(corpus_preprocessed, removeNumbers)

    # Convert text to lowercase.
    corpus_preprocessed <-
        tm_map(corpus_preprocessed, content_transformer(tolower))

    # Strip whitespace from text.
    corpus_preprocessed <- tm_map(corpus_preprocessed, stripWhitespace)

    # Stem the text.
    # corpus_preprocessed <- tm_map(corpus_preprocessed, stemDocument)

    # Remove stopwords.
    corpus_preprocessed <-
        tm_map(corpus_preprocessed, removeWords, stopwords("en"))

    # Return value.
    return(corpus_preprocessed)

}

```

Data cleaning involves transforming the raw text in the corpus into a format more suitable for automated manipulation.  The tm package provides numerous functions for such transformations (see Feinerer et al., 2008, p. 9).  For this package, the texts were converted to lower case, stripped of whitespace, and common stopwords (i.e., words so common that they contain little information; see Feinerer et al., 2008, pp. 25-26) were removed.






### Data Processing

According to Wikipedia (N-gram, n.d.), "an *n*-gram is a contiguous sequence of n items from a given sequence of text or speech."  This package takes a key word or phrase, matches that key to the most frequent *n*-1 term found in a TDM of *n*-word terms, and returns the *n*th word of that item.

Of course, not all possible words or phrases exist in the corpus from which the TDM was derived.  For this reason, a simplified Katz's back-off model is used, which backs off to smaller *n*-grams when a key is not found in the larger *n*-gram.  The maximum *n*-gram handled is a trigram.  The word returned is the match found in the largest *n*-gram where the key is found.  When the key is not found in the unigram, the most common word in the corpus "will" is returned.  This function is demonstrated using a Shiny app hosted on shinyapps.io 

### Conclusion

This report has shown features the R package "wordprediction".  It was designed using samples of 1000 words each from a corpus of collections English words.  The corpus has a large range of words and word frequencies with a number of important words.  The sample size was large enough to satisfy two laws from linguistics concerning large corpora.  If the desired sample size is deemed to be too small, it can easilly be increased by editing the source code.  As is, this analysis ran rather quickly on a home computer and on shinyapps.io.  As shown in a demonstration, all phrases and words submitted to the function "katz_backoff_model" result in a prediction in the form of a single word returned.

### References

Christensen, H. (n.d.). *HC Corpora*. Retrieved from [http://www.corpora.heliohost.org](http://www.corpora.heliohost.org)

Explore Corpus Term Frequency Characteristics [Computer software]. (n.d.). Retrieved from http://www.inside-r.org/packages/cran/tm/docs/Zipf_n_Heaps

Feinerer, I., Hornik, K., & Artifex Software, Inc. (2015, July 2). Text Mining Package [Computer software]. Retrieved from [http://tm.r-forge.r-project.org](http://tm.r-forge.r-project.org)

Feinerer, I., Hornik, K., & Meyer, D. (2008). *Text mining infrastructure in R. Journal of Statistical Software, 25*(5), 1â€“54. [http://doi.org/citeulike-article-id:2842334](http://doi.org/citeulike-article-id:2842334)

Leek, J., Peng, R., Caffo, B., & Johns Hopkins University (n.d.). *Data Science Capstone*, Coursera. Retrieved from [https://www.coursera.org/learn/data-science-project/](https://www.coursera.org/learn/data-science-project/)



